# -*- coding: utf-8 -*-
"""emotion_analizer_by_words.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iX7Zps-y1v522ZJyAHzN4-UKOoKjRFHC

importação dos dados
"""

import pandas as pd

uri_comentarios = 'https://media.githubusercontent.com/media/fabriciobedin/machine-learning/master/emotion_analyzer_by_words/imdb-reviews-pt-br.csv'
comentarios = pd.read_csv(uri_comentarios)
comentarios.head()

"""alterando coluna de sentiment pra classificação, onde o sentimento negativo vai se tornar 0 e o positivo vai se tornar 1"""

classificacao = comentarios["sentiment"].replace(["neg", "pos"], [0,1])
comentarios["classificacao"] = classificacao
comentarios.head()

"""utilizando o Count Vectorizer pra criar um bag of words, ele vai separar todas as palavras encontradas em cada frase."""

from sklearn.feature_extraction.text import CountVectorizer

# Exemplo
textos = ["Assisti um filme ótimo", "Assisti um filme péssimo"]
vetorizar = CountVectorizer(lowercase = False)
bag_of_words = vetorizar.fit_transform(textos)
vetorizar.get_feature_names()

"""criando um datatable a partir desses dados de teste pra entender o funcionamento. Ao invés de armazenar os valores 0 e ficar ocupando memória, ele armazena um valor NaN que seria um valor nulo."""

vetorizado = pd.SparseDataFrame(
    bag_of_words,
    columns = vetorizar.get_feature_names()
)
vetorizado

"""Iniciando a classificação de sentimento com base na planilha importada."""

vetorizar = CountVectorizer(lowercase = False, max_features = 50)
bag_of_words = vetorizar.fit_transform(comentarios["text_pt"])
print(bag_of_words.shape)

"""separando dados entre treino e teste com sklearn.model_selection e exibindo proporção separada"""

from sklearn.model_selection import train_test_split


treino, teste, classe_treino, classe_teste = train_test_split(bag_of_words,
                                                              comentarios["classificacao"],
                                                              random_state = 42)
print('Treino: {treino}, Teste: {teste}'.format(treino = treino.shape, teste = teste.shape))

"""criando modelo linear com logistic regression"""

from sklearn.linear_model import LogisticRegression

regressao_logistica = LogisticRegression(solver = "lbfgs")
regressao_logistica.fit(treino, classe_treino)
previsao_teste = regressao_logistica.predict_proba(teste)
print(previsao_teste)

import numpy as np

previsao_teste_bol = previsao_teste[:,1] >= 0.5
previsao_teste_int = previsao_teste_bol.astype(np.int)

print('Previsão teste bol: {bol}\nPrevisão teste int: {int}'
      .format(bol = previsao_teste_bol, int = previsao_teste_int))

"""criando porcentagem de acerto do algorítmo"""

from sklearn.metrics import accuracy_score
accuracy_teste = accuracy_score(classe_teste, previsao_teste_int)
print('Accuracy: {accuracy}%'.format(accuracy = round(accuracy_teste*100, 2)))

"""visualizando palavras com wordcloud, vai exibir as palavras em um tamanho diferente conforme quantidade de vezes que elas aparecem."""

# %matplotlib inline

from wordcloud import WordCloud

todas_palavras = ' '.join([texto for texto in comentarios["text_pt"]])

nuvem_palavras = WordCloud(width = 800, height = 500, max_font_size = 110,
                          collocations = False).generate(todas_palavras)

import matplotlib.pyplot as plt

plt.figure(figsize= (10, 7))
plt.imshow(nuvem_palavras, interpolation = 'bilinear')
plt.axis('off')
plt.show()

"""utilizando o query pra separar os comentários positivos dos negativos"""

comentarios_positivos = comentarios.query("sentiment == 'pos'")
comentarios_negativos = comentarios.query("sentiment == 'neg'")

"""exibindo wordcloud apenas com palavras positivas"""

palavras_pos = ' '.join([texto for texto in comentarios_positivos["text_pt"]])
nuvem_palavras = WordCloud(width=800, height=500, max_font_size=110,
                           collocations=False).generate(palavras_pos)
plt.figure(figsize=(10,7))
plt.imshow(nuvem_palavras, interpolation='bilinear')
plt.axis('off')
plt.show()

"""exibindo wordcloud apenas com palavras negativas"""

palavras_neg = ' '.join([texto for texto in comentarios_negativos["text_pt"]])
nuvem_palavras = WordCloud(width=800, height=500, max_font_size=110,
                           collocations=False).generate(palavras_neg)
plt.figure(figsize=(10,7))
plt.imshow(nuvem_palavras, interpolation='bilinear')
plt.axis('off')
plt.show()

"""Utilizando um recurso de tokenização do NLTK. Ele separa uma frase em palavras, levando em consideração algum tipo de separador. No exemplo abaixo vou utilizar uma frase e separar por espaços em branco."""

from nltk import tokenize

frase = "Bem vindo ao mundo do PLN"
token_espaco = tokenize.WhitespaceTokenizer()
token_frase = token_espaco.tokenize(frase)
print(token_frase)

"""agora vamos aplicar ao nosso modelo pra separar as palavras e exibir a frequência que elas aparecem nos comentários"""

import nltk

todas_palavras = ' '.join([texto for texto in comentarios["text_pt"]])
frequencia = nltk.FreqDist(token_espaco.tokenize(todas_palavras))
df_frequencia = pd.DataFrame({"Palavras": list(frequencia.keys()),
                             "Frequência": list(frequencia.values())})
df_frequencia = df_frequencia.nlargest(columns = "Frequência", n = 13)
df_frequencia

"""utilizando o SeaBorn pra criar um gráfico dessas palavras e frequencia."""

import seaborn as sns

plt.figure(figsize=(12,8))
ax = sns.barplot(data = df_frequencia, x= "Palavras", y = "Frequência", color = 'gray')
ax.set(ylabel = "Contagem")
plt.show()

"""como vocês podem ver, as palavras mais utilizadas não vão nos ajudar a distinguir o sentimento do comentário, devemos eliminar essas palavras. Vamos utilizar o recurso stopwords do NLTK.

Abaixo vamos criar uma função que percore cada comentário, separa as palavras e testa se é uma palavra relevante ou não, e gera uma nova coluna recriando o comentário sem as palavras irrelevantes
"""

import nltk
nltk.download('stopwords')

palavras_irrelevantes = nltk.corpus.stopwords.words("portuguese")
frase_processada = list()

for comentario in comentarios["text_pt"]:
    nova_frase = list()
    palavras_texto = token_espaco.tokenize(comentario)
    for palavra in palavras_texto:
        if palavra not in palavras_irrelevantes:
            nova_frase.append(palavra)
    frase_processada.append(' '.join(nova_frase))
    
comentarios["tratamento_1"] = frase_processada
comentarios.head()

"""definindo uma função pra criar a nuvem de palavras"""

def nuvem_palavras(texto, coluna_texto, sentimento):
    texto_separado = texto.query("sentiment == '{sentimento}'".format(sentimento = sentimento))
    todas_palavras = ' '.join([texto for texto in texto_separado[coluna_texto]])
    nuvem_palavras = WordCloud(width = 800, height = 500,
                                max_font_size=110, collocations=False).generate(todas_palavras)
    plt.figure(figsize=(10,7))
    plt.imshow(nuvem_palavras, interpolation='bilinear')
    plt.axis('off')
    plt.show()

"""exibindo nuvem de palavras negativas depois de ter removido as palavras desnecessárias"""

nuvem_palavras(comentarios, 'tratamento_1', 'neg')

"""exibindo núvem de palavras positivas depois de ter removido as desnecessárias"""

nuvem_palavras(comentarios, 'tratamento_1', 'pos')

"""criando função pra criar gráfico pareto"""

def grafico_pareto(texto, coluna_texto, quantidade):
    todas_palavras = ' '.join([texto for texto in texto[coluna_texto]])
    frequencia = nltk.FreqDist(token_espaco.tokenize(todas_palavras))
    df_frequencia = pd.DataFrame({"Palavra": list(frequencia.keys()),
                                 "Frequência": list(frequencia.values())})
    df_frequencia = df_frequencia.nlargest(columns = "Frequência", n = quantidade)
    plt.figure(figsize=(12,8))
    ax = sns.barplot(data = df_frequencia, x= "Palavra", y = "Frequência", color = 'gray')
    ax.set(ylabel = "Contagem")
    plt.show()

"""exibindo gráfico de palavras depois de ter removido as desnecessárias"""

grafico_pareto(comentarios, "tratamento_1", 13)

"""criando função pra classificar o texto com a regreção logística"""

def classificar_texto(texto, coluna_texto, coluna_classificacao):
    vetorizar = CountVectorizer(lowercase=False, max_features=50)
    bag_of_words = vetorizar.fit_transform(texto[coluna_texto])
    treino, teste, classe_treino, classe_teste = train_test_split(bag_of_words,
                                                                 texto[coluna_classificacao],
                                                                 random_state = 42)
    
    regressao_logistica = LogisticRegression(solver="lbfgs")
    regressao_logistica.fit(treino, classe_treino)
    
    return regressao_logistica.score(teste, classe_teste)

"""exibindo a porcentagem de acerto após remover as palavras desnecessárias"""

accuracy_teste = classificar_texto(comentarios, "tratamento_1", "classificacao")
print('Accuracy: {accuracy}%'.format(accuracy = round(accuracy_teste*100, 2)))

"""otimizando a análise"""

