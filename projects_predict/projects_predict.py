# -*- coding: utf-8 -*-
"""machine_learning_ex_jobs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_KZn8Pt89MBeKxyp1Bz0iNL90fTG_JJL
"""

!pip install seaborn==0.9.0

"""**importando dados**"""

import pandas as pd

uri = "https://gist.githubusercontent.com/guilhermesilveira/1b7d5475863c15f484ac495bd70975cf/raw/16aff7a0aee67e7c100a2a48b676a2d2d142f646/projects.csv"
dados = pd.read_csv(uri)
dados.head()

"""**renomeando tabela**"""

a_renomear = {
    'expected_hours' : 'horas_esperadas',
    'price' : 'preco',
    'unfinished' : 'nao_finalizado'
}
dados = dados.rename(columns = a_renomear)
dados.head()

"""**invertendo dados de não finalizado para finalizado**"""

troca = {
    0 : 1,
    1 : 0
}
dados['finalizado'] = dados.nao_finalizado.map(troca)
dados.head()

"""**visualizando últimos 5 dados**"""

dados.tail()

"""**desenhar em um eixo as horas_esperadas e em outro o preco**

Para este método, precisamos passar o parâmetro referente às coluna x (horas_esperadas) e y (preco). Por fim, devemos passar os dados, de onde vem nosso dataframe do Pandas.
"""

import seaborn as sns

sns.scatterplot(x="horas_esperadas", y="preco", data=dados)

"""**Adicionando uma cor a um tipo de dado, pra ver a diferença entre o que foi finalizado ou não com base no preço e horas**"""

sns.scatterplot(x="horas_esperadas", y="preco", hue="finalizado", data=dados)

"""**gerando gráfico separado**"""

sns.relplot(x="horas_esperadas", y="preco", hue="finalizado", col="finalizado", data=dados)

"""**separando dados pra modelagem**"""

x = dados[['horas_esperadas', 'preco']]
y = dados['finalizado']

"""**modelo com os dados de treino e teste separados e com taxa de acerto**

o seed foi adicionado pra ele não ficar pegando dados de forma randominca, ele sempre pegará o mesmo conjunto de dados pra fazer os testes
"""

from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score

np.random.seed(20)

treino_x, teste_x, treino_y, teste_y = train_test_split(x, y, test_size = 0.25, stratify = y)
print("Treinaremos com %d elementos e testaremos com %d elementos" % (len(treino_x), len(teste_x)))

modelo = LinearSVC()
modelo.fit(treino_x, treino_y)
previsoes = modelo.predict(teste_x)

acuracia = accuracy_score(teste_y, previsoes) * 100
print("A acurácia foi %.2f%%" % acuracia)

"""**criando um algoritmo "burro" que vai colocar 1 em todos os casos pra ver se ele possui uma taxa de acerto maior que o algorítmo criado**"""

import numpy as np
previsoes_de_base = np.ones(540)
acuracia = accuracy_score(teste_y, previsoes_de_base) * 100
print("A acurácia do algoritmo baseline foi %.2f%%" % acuracia)

"""**grafico do teste**"""

sns.scatterplot(x="horas_esperadas", y="preco", hue=teste_y, data=teste_x)

"""Do conjunto teste_x, coletaremos o valor mínimo (min()) para horas_esperadas, e atribuiremos o nome x_min a esse valor. Faremos um procedimento similar para x_max, mas dessa vez coletando o valor máximo. Repetiremos essa estrutura para teste_y, mas agora trabalhando com preco mínimo e máximo. Por fim, imprimiremos o resultado.

Na tela, teremos quatro valores - respectivamente, os mínimos e máximos de x e y.
"""

x_min = teste_x.horas_esperadas.min()
x_max = teste_x.horas_esperadas.max()
y_min = teste_x.preco.min()
y_max = teste_x.preco.max()
print(x_min, x_max,y_min,y_max)

"""Por fim, adicionaremos o método arange() da biblioteca do Numpy."""

pixels = 100
np.arange(x_min, x_max, (x_max - x_min)/pixels)

"""A partir de 1, foram realizadas somas de 0.99 até chegar ao número99.01, dividindo proporcionalmente o espaço do eixo_x no gráfico. Faremos o mesmo procedimento para o eixo_y, aproveitando para nomear cada uma dessas operações:"""

pixels = 100
eixo_x = np.arange(x_min, x_max, (x_max - x_min)/ pixels)
eixo_y = np.arange(y_min, y_max, (y_max - y_min)/ pixels)

"""Agora a ideia é criarmos um grid entre os dois eixos, multiplicando as opções de acordo com as diferentes possibilidade de custo de projeto e tempo de execução. Temos na biblioteca do Numpy o np.meshgrid(), que consegue mesclar um grid . Esse método recebe como parâmetros o eixo_x e eixo_y, e devolve o conteúdo é xx e yy. Vamos analisar xx:"""

xx, yy = np.meshgrid(eixo_x, eixo_y)
xx

"""Com o código estruturado desta maneira, o que temos é a repetição do eixo x 100 vezes, e o mesmo ocorre com eixo y. Ou seja, ainda não mesclamos realmente esses dados. Para fazermos isso, usaremos o xx.ravel():"""

xx, yy = np.meshgrid(eixo_x, eixo_y)
xx.ravel()

"""Faremos o mesmo procedimento para yy, e então concatenaremos xx e yy por meio de np.c_. Essa operação nos devolverá os pontos."""

xx, yy = np.meshgrid(eixo_x, eixo_y)
pontos = np.c_[xx.ravel(), yy.ravel()]
pontos

"""Agora, nosso trabalho é, a partir do modelo, fazer as previsões (predict() para todos esses pontos. Como resultado, teremos uma série de números, que são as classificações. Chamaremos essas classificações de Z."""

Z = modelo.predict(pontos)
Z.shape
xx.shape

"""Precisamos redimensionar (reshape) o array de 10000 de acordo com xx."""

Z = modelo.predict(pontos)
Z = Z.reshape(xx.shape)
Z

"""Finalmente, poderemos plotar esses dados. Para isso, usaremos a biblioteca Matplotlib, cujas importações costumam ser plt. Novamente, queremos que os pontos sejam espalhados (scatter()), mas não usaremos o scatterplot() do Seaborn pois, nesse caso, queremos um controle mais refinado dessa plotagem.

O método scatter() deverá receber teste_x.horas_esperadas e teste_x.preco. Em seguida, definiremos a cor por meio do argumentoc= recebendo teste_y.
"""

plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(teste_x.horas_esperadas, teste_x.preco, c=teste_y, s=1)

"""**testando com SVC**"""

from sklearn.svm import SVC

SEED = 5
np.random.seed(SEED)
treino_x, teste_x, treino_y, teste_y = train_test_split(x, y, test_size = 0.25,
                                                         stratify = y)
print("Treinaremos com %d elementos e testaremos com %d elementos" % (len(treino_x), len(teste_x)))

modelo = SVC()
modelo.fit(treino_x, treino_y)
previsoes = modelo.predict(teste_x)

acuracia = accuracy_score(teste_y, previsoes) * 100
print("A acurácia foi %.2f%%" % acuracia)

"""No módulo sklearn.smv, teremos diversas variações de algorítimos baseados em uma única ideia: Suport Vector Machines, isto é, SVM. Estávamos utilizando anteriormente o LinearSVC, ou Suport Vector Classification, que é baseado em relacionamentos lineares."""

x_min = teste_x.horas_esperadas.min()
x_max = teste_x.horas_esperadas.max()
y_min = teste_x.preco.min()
y_max = teste_x.preco.max()

pixels = 100
eixo_x = np.arange(x_min, x_max, (x_max - x_min) / pixels)
eixo_y = np.arange(y_min, y_max, (y_max - y_min) / pixels)

xx, yy = np.meshgrid(eixo_x, eixo_y)
pontos = np.c_[xx.ravel(), yy.ravel()]

Z = modelo.predict(pontos)
Z = Z.reshape(xx.shape)

import matplotlib.pyplot as plt

plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(teste_x.horas_esperadas, teste_x.preco, c=teste_y, s=1)

"""Analisando a plotagem, percebemos que de fato foram realizados relacionamentos não lineares. Há pequenos agrupamentos de cor diferente que representam as previsões do algorítimo para projetos que terão valor 1, e o restante será 0. Não estamos tendo um palpite muito inteligente.

Isso o ocorre devido a discrepância entre os eixos: em X, teremos valores de 0 a 100, e em Y de 0 a 30000. Esses algorítimos são muito suscetíveis a escala, e darão menos valor para variações menores, como é o caso de X.

Queremos elaborar uma nova escala a partir dos valores de X para treino. Para tanto, criaremos um StandardScaler(), que será atribuído à uma variável scaler, e treinaremos esse processo baseado em treino_x. Em seguida, transformaremos treino_x em um novo treino_x que já está na nova escala. Em seguida, repetiremos o processo para teste_x.

Desse modo, todas as features serão utilizadas para treinar o processo de escala, e por fim, atualizadas de acordo com as novas medidas. Já sabemos que não é uma boa prática sobrescrever variáveis, pois podemos nos perder em meio às nomeclaturas. Portanto, renomearemos os dados "crus" (os originais) adicionando o termo raw - ou seja, raw_treino_x e raw_teste_x.
"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

SEED = 5
np.random.seed(SEED)
raw_treino_x, raw_teste_x, treino_y, teste_y = train_test_split(x, y, test_size = 0.25,
                                                         stratify = y)
print("Treinaremos com %d elementos e testaremos com %d elementos" % (len(treino_x), len(teste_x)))

scaler = StandardScaler()
scaler.fit(raw_treino_x)
treino_x = scaler.transform(raw_treino_x)
teste_x = scaler.transform(raw_teste_x)

modelo = SVC()
modelo.fit(treino_x, treino_y)
previsoes = modelo.predict(teste_x)

acuracia = accuracy_score(teste_y, previsoes) * 100
print("A acurácia foi %.2f%%" % acuracia)

data_x = teste_x[:,0]
data_y = teste_x[:,1]

x_min = data_x.min()
x_max = data_x.max()
y_min = data_y.min()
y_max = data_y.max()

pixels = 100
eixo_x = np.arange(x_min, x_max, (x_max - x_min) / pixels)
eixo_y = np.arange(y_min, y_max, (y_max - y_min) / pixels)

xx, yy = np.meshgrid(eixo_x, eixo_y)
pontos = np.c_[xx.ravel(), yy.ravel()]

Z = modelo.predict(pontos)
Z = Z.reshape(xx.shape)

import matplotlib.pyplot as plt

plt.contourf(xx, yy, Z, alpha=0.3)
plt.scatter(data_x, data_y, c=teste_y, s=1)